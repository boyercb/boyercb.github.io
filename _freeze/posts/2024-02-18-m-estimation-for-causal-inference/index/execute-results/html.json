{
  "hash": "0da7fcf97c0e5e3cc787b6feb51257e9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"M-estimation for causal inference in R\"\nsubtitle: | \n    TLDR: Why a good sandwich can solve so many of life's problems.\ndate: \"2024-02-16\"\ncategories: [causal inference, m-estimation, code]\n---\n\n\n\n\nIn this post \n\n# A simple causal example\nTo fix concepts, let's start with a simple example where we're interested in estimating the effect of a single binary time-fixed treatment, $A$, on outcome $Y$. Define $Y^a$ as the potential outcome under a hypothetical intervention which sets $A$ to $a$. Our interest is in causal contrasts such as the average treatment effect, i.e.\n$$ \n\\psi \\equiv E(Y^{a=1} - Y^{a=0}).\n$$\nBecause the focus of this post is on estimation, let's assume the ideal observational setting in which measured covariates, $L$, are sufficient to control confounding. Under standard identifiability conditions, namely:\n\n1. Exchangeability: $Y^{a} \\perp\\!\\!\\!\\perp A \\mid L$\n2. Consistency: $Y^{a} = Y \\text{  if } A = a$\n3. Positivity: $1 > \\Pr(A = a \\mid L = l) > 0$\n\nit can be shown that $\\psi$ is identified by \n$$\n\\psi_{om} \\equiv E\\{E(Y | A = 1, L)\\} - E\\{E(Y | A = 0, L)\\},\n$$\nand, equivalently, by\n$$\n\\psi_{ipw} \\equiv E\\left\\{\\frac{I(A = 1)}{\\Pr(A = 1 \\mid L)} Y\\right\\} - E\\left\\{\\frac{I(A = 0)}{\\Pr(A = 0 \\mid L)} Y\\right\\}.\n$$\n\nTo simplify, define $\\mu_a(L) = E(Y | A = a, L)$ and $e(L) = \\Pr(A = 1 | L)$. Each expression suggests a corresponding plug-in estimator, i.e.\n$$\n\\hat{\\psi}_{om} \\equiv \\dfrac{1}{n} \\sum_{i=1}^n \\hat{\\mu}_1(L_i) - \\hat{\\mu}_0(L_i),\n$$\nand\n$$\n\\hat{\\psi}_{ipw} = \\dfrac{1}{n} \\sum_{i=1}^n \\dfrac{A_i}{\\hat{e}(L_i)}Y_i  - \\sum_{i=1}^n \\dfrac{1 - A_i}{1 - \\hat{e}(L_i)}Y_i,\n$$\nwhere the first is termed the outcome model or regression estimator because it relies on a model for $E(Y | A = a, L)$ and the second is the inverse probability weighting estimator because it relies on a model for $\\Pr(A = 1 | L)$. \n\nHernan and Robins describe, in detail, the necessary analysis steps to obtain estimates $\\hat{\\psi}_{om}$ and $\\hat{\\psi}_{ipw}$. Briefly, the outcome regression estimator is obtained by  \n\n1. Regressing the outcome, $Y$, on covariates, $L$, either a) separately within the subset with $A = 1$ and $A = 0$, or b) by pooling into one model of $Y$ on $L$ and $A$. \n2. Create two copies of the dataset and artificially set $A = 1$ for all individuals in one and set $A = 0$ for all individuals in the other.\n3. Obtain predicted values $\\hat{\\mu}_1(L_i)$ and $\\hat{\\mu}_0(L_i)$ by applying the model in step 1 to the observed values of $L_i$ in each dataset.\n4. Take the mean of these predicted values and subtract them to obtain $\\hat{\\psi}_{om}$.\n\nand the inverse probability weighting estimator may be obtained by\n\n1. Regressing the treatment, $A$, on covariates, $L$, using, for instance, a logistic model. \n2. Obtain propensity scores $\\hat{e}(L_i)$ from predicted values of logistic model.\n3. For each individual form weights based on the inverse probability of the treatment they received\n$$\nW_i = \\dfrac{A_i}{\\hat{e}(L_i)}\n$$\n4. Calculate the weighted mean and take the difference.\n\n# The problem\nImplementing \n\n\n::: {.cell}\n\n:::\n\nIn their book, Hernan and Robins suggest variance estimates can be obtained via the bootstrap. However, there are at least two issues. First the bootstrap may be computationally intensive especially as problems become more complex (e.g. once we consider time-varying treatments).\n\n# The basics of M-estimation\nThe basic set up is as follows. Let $\\theta$ be a finite dimensional parameter vector. An M-estimator, $\\hat{\\theta}$, for this vector is the solution to sample moment equation\n$$\n\\sum_{i=1}^n \\psi(O_i; \\hat{\\theta}) = 0\n$$\nwhere $\\psi(O_i; \\hat{\\theta})$ is often referred to as an estimating function. Under standard regularity conditions, the asymptotic distribution of $\\hat{\\theta}$ is given by \n$$\n\\sqrt{n}(\\theta - \\hat{\\theta}) \\overset{d}{\\longrightarrow} N(0, \\mathbb V(\\hat{\\theta}))\n$$\nwhere the asymptotic variance, $\\mathbb V_n(\\hat{\\theta})$, can be estimated from\n$$\n\\mathbb V_n(\\hat{\\theta}) = \\mathbb B_n(\\hat{\\theta})^{-1} \\mathbb F_n(\\hat{\\theta})\\{\\mathbb B_n(\\hat{\\theta})^{-1}\\}^T\n$$\nwith \n$$\n\\mathbb B_n(\\hat{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n \\left\\{-\\dfrac{\\partial\\psi(O_i; \\hat{\\theta})}{\\partial\\theta}\\right\\}\n$$\nand \n$$\n\\mathbb F_n(\\hat{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n \\left\\{\\partial\\psi(O_i; \\hat{\\theta})\\partial\\psi(O_i; \\hat{\\theta})^T\\right\\}\n$$\nIn M-estimation parlance $\\mathbb V_n(\\hat{\\theta})$ is referred to as the empirical sandwich variance estimator where $\\mathbb B_n(\\hat{\\theta})$ is the \"bread\" and $\\mathbb F_n(\\hat{\\theta})$ is the \"meat\". While analytical solutions can often be derived for the asymptotic variance for a given set of estimating functions, an advantage of M-estimation is that the variance can also be obtained numerically and therefore can be easily extended to any arbitrary set of estimating functions.\n\nThe beauty of M-estimation is that many common estimation problems can be posed as an estimating function or sequence of estimating functions. Estimating functions can be ``stacked'' \n\n1. Can I re-express my estimator as a set of stacked estimating equations?\n2. Do I meet the \n\n# The `geex` package\n\n### Outcome regression\nWe can convert $\\hat{\\psi}_{om}$ into the following set of stacked estimating equations\n$$\n\\psi(O; \\beta_0, \\beta_1, \\psi) = \\begin{bmatrix}\nA \\{Y - \\mu_1(L;\\beta_1)\\} \\\\\n(1 -A) \\{Y - \\mu_0(L;\\beta_0)\\} \\\\\n\\mu_1(L;\\beta_1) - \\mu_0(L;\\beta_1) - \\psi\n\\end{bmatrix}\n$$\nwhere the first two are the score equations for the models $\\mu_1(L;\\beta_1)$ and $\\mu_0(L;\\beta_1)$ respectively and the last obtains the estimate of the difference in predicted values. \n\nIn the `geex` package the stacked estimating functions for outcome regression can be implemented as follows. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nestfun_or <- function(data, models) {\n  # grab scores \n  m0_scores <- grab_psiFUN(models$m0, data)\n  m1_scores <- grab_psiFUN(models$m1, data)\n\n  # design matrices\n  X0 <- grab_design_matrix(data, grab_fixed_formula(models$m0))\n  X1 <- grab_design_matrix(data, grab_fixed_formula(models$m1))\n\n  # parameter indexes\n  ii0 <- 1:ncol(X0)\n  ii1 <- (ncol(X0) + 1):(ncol(X0) + ncol(X1))\n\n  A <- data$A\n\n  # stacked equations\n  function(theta) {\n    c(m0_scores(theta[ii0]) * (A == 0), \n      m1_scores(theta[ii1]) * (A == 1),\n      X1 %*% theta[ii1] - X0 %*% theta[ii0] - theta[length(theta)])\n  }\n}\n```\n:::\n\n\n### Inverse probability weighting\nUsing some probability theory we can show that $\\psi_{om}$ is equal to \n$$\n\\psi_{ipw} \\equiv E\\left\\{\\frac{I(A = 1)}{\\Pr(A = 1 \\mid L)} Y\\right\\} - E\\left\\{\\frac{I(A = 0)}{\\Pr(A = 0 \\mid L)} Y\\right\\}\n$$\nand simple plug-in estimator\n\n$$\n\\hat{\\psi}_{ipw} = \\sum_{i=1}^n \\dfrac{A_i}{\\hat{e}(L_i)}Y_i  - \\sum_{i=1}^n \\dfrac{1 - A_i}{1 - \\hat{e}(L_i)}Y_i.\n$$\n\nThe variance is complicated by the estimation of the nuisance terms $\\hat{\\mu}_1(L)$ and $\\hat{\\mu}_0(L)$. \n\n$$\n\\psi(O; \\alpha, \\psi) = \\begin{bmatrix}\n\\{A - e(L;\\alpha)\\} L \\\\\n\\dfrac{A}{\\hat{e}(L;\\alpha)}Y  - \\dfrac{1 - A}{1 - \\hat{e}(L;\\alpha)}Y - \\psi\n\\end{bmatrix}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nestfun_or <- function(data, models) {\n  # grab scores \n  e_scores <- grab_psiFUN(models$e, data)\n\n  # design matrix\n  X <- grab_design_matrix(data, grab_fixed_formula(models$e))\n\n  # parameter indexes\n  ii <- 1:ncol(X)\n\n  A <- data$A\n  Y <- data$Y\n\n  # stacked equations\n  function(theta) {\n    e <- plogis(X %*% theta[ii])\n    c(e_scores(theta[ii]), \n      (A * Y / e) - ((1 - A) * Y / (1 - e)) - theta[length(theta)])\n  }\n}\n```\n:::\n\n\n### G-estimation\n\n\n$$\n\\begin{aligned}\nE\\{Y - E(Y \\mid A = a, X)\\} = 0 \\\\\nE[\\{A - E(A | X)\\}\\{Y - \\psi A\\}] = 0\n\\end{aligned}\n$$ \n\n# Putting it all together\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(geex)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'geex'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:methods':\n\n    show\n```\n\n\n:::\n\n```{.r .cell-code}\ngendata <- function(n) {\n  L <- MASS::mvrnorm(\n    n = n,\n    mu = rep(0, 3),\n    Sigma = matrix(\n      data = c(1, 0.3, 0.3,\n               0.3, 1, 0.3,\n               0.3, 0.3, 1), \n      nrow = 3,\n      ncol = 3,\n      byrow = TRUE)\n  )   \n  A <- rbinom(n, 1, plogis(-2 + L[, 1] + sqrt(exp(L[, 2]))+ L[, 3]^2))\n  Y <- rnorm(n, -A + 0.5 * (L[, 1] + L[, 2] + L[, 3] + L[, 1] * L[, 3]))\n  \n  colnames(L) <- paste0(\"L\", 1:3)\n  mat <- data.frame(L, A, Y)\n  return(mat)\n}\n\nestfun <- function(data, models) {\n  m0 <- grab_psiFUN(models$m0, data)\n  m1 <- grab_psiFUN(models$m1, data)\n  X0 <- grab_design_matrix(data, rhs_formula = grab_fixed_formula(models$m0))\n  X1 <- grab_design_matrix(data, rhs_formula = grab_fixed_formula(models$m1))\n  ii0 <- 1:ncol(X0)\n  ii1 <- (ncol(X0) + 1):(ncol(X0) + ncol(X1))\n  \n  A <- data$A\n  function(theta) {\n    c(m0(theta[ii0]) * (A == 0), \n      m1(theta[ii1]) * (A == 1),\n      X1 %*% theta[ii1] - X0 %*% theta[ii0] - theta[length(theta)] )\n  }\n}\n\nd <- gendata(1000)\n\nmodels <- list(\n  m0 = glm(formula = Y ~ L1 + L2 + L3 + L1:L3, \n           family = gaussian,\n           subset = A == 0,\n           data = d),\n  m1 = glm(formula = Y ~ L1 + L2 + L3 + L1:L3, \n           family = gaussian,\n           subset = A == 1,\n           data = d)\n)\n\nnparms <- sum(sapply(models, function(x) length(coef(x)))) + 1\n\nres <- m_estimate(\n  estFUN = estfun,\n  data = d,\n  root_control = setup_root_control(start = rep(0, nparms)),\n  outer_args = list(models = models)\n)\n\nc(\n  roots(res)[11],\n  roots(res)[11] - 1.96 * sqrt(vcov(res)[11,11]),\n  roots(res)[11] + 1.96 * sqrt(vcov(res)[11,11])\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.9601858 -1.1033235 -0.8170480\n```\n\n\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
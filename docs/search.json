[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Bayesian measurement error models for longitudinal data\n\n\nTLDR: Why a good sandwich can solve so many of life’s problems. \n\n\n\nmeasurement\n\nBayes\n\nStan\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nChristopher Boyer\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\nChristopher Boyer, Harlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nThe many faces of the g-formula\n\n\nAnd how to implement them from scratch using R. \n\n\n\ncausal inference\n\ncode\n\ng-formula\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\nChristopher Boyer\n\n\n\n\n\n\n\n\n\n\n\n\nThe reasons people start treatment are not the same as the reasons they stop\n\n\nAnd what this implies for causal inference for sustained treatment strategies. \n\n\n\ncausal inference\n\ncode\n\ng-formula\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\nChristopher Boyer\n\n\n\n\n\n\n\n\n\n\n\n\nM-estimation for causal inference in R\n\n\nTLDR: Why a good sandwich can solve so many of life’s problems. \n\n\n\ncausal inference\n\nm-estimation\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\nChristopher Boyer\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nChristopher Boyer, Tristan O’Malley\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Christopher B. Boyer",
    "section": "",
    "text": "I am currently an Assistant Professor at the Cleveland Clinic Lerner College of Medicine of Case Western Reserve University and a staff Biostatistician at the Lerner Research Institute at the Cleveland Clinic.\nMy research focuses on methods for improving causal estimation and inference in observational and randomized studies. In the past, this has included work counterfactual prediction, postmarket vaccine evaluation, target trial emulation, and interference/spillover and placebo effects in randomized trials. At the Clinic, I collaborate with investigators at the Center for Value Based Care Research leveraging electronic medical records databases to evaluate comparative effectiveness of primary care and hospital-based interventions and as a platform for novel trial design to improve medical decision-making at the point of care.\n\nEducation\n\nPhD, Epidemiology\n\nHarvard University, 2023.\n\nMPH, Epidemiology\n\nColumbia University, 2015.\n\nBS, Mechanical Engineering\n\nWright State University, 2010."
  },
  {
    "objectID": "posts/2024-02-18-m-estimation-for-causal-inference/index.html",
    "href": "posts/2024-02-18-m-estimation-for-causal-inference/index.html",
    "title": "M-estimation for causal inference in R",
    "section": "",
    "text": "In this post\n\nA simple causal example\nTo fix concepts, let’s start with a simple example where we’re interested in estimating the effect of a single binary time-fixed treatment, \\(A\\), on outcome \\(Y\\). Define \\(Y^a\\) as the potential outcome under a hypothetical intervention which sets \\(A\\) to \\(a\\). Our interest is in causal contrasts such as the average treatment effect, i.e. \\[\n\\psi \\equiv E(Y^{a=1} - Y^{a=0}).\n\\] Because the focus of this post is on estimation, let’s assume the ideal observational setting in which measured covariates, \\(L\\), are sufficient to control confounding. Under standard identifiability conditions, namely:\n\nExchangeability: \\(Y^{a} \\perp\\!\\!\\!\\perp A \\mid L\\)\nConsistency: \\(Y^{a} = Y \\text{  if } A = a\\)\nPositivity: \\(1 &gt; \\Pr(A = a \\mid L = l) &gt; 0\\)\n\nit can be shown that \\(\\psi\\) is identified by \\[\n\\psi_{om} \\equiv E\\{E(Y | A = 1, L)\\} - E\\{E(Y | A = 0, L)\\},\n\\] and, equivalently, by \\[\n\\psi_{ipw} \\equiv E\\left\\{\\frac{I(A = 1)}{\\Pr(A = 1 \\mid L)} Y\\right\\} - E\\left\\{\\frac{I(A = 0)}{\\Pr(A = 0 \\mid L)} Y\\right\\}.\n\\]\nTo simplify, define \\(\\mu_a(L) = E(Y | A = a, L)\\) and \\(e(L) = \\Pr(A = 1 | L)\\). Each expression suggests a corresponding plug-in estimator, i.e. \\[\n\\hat{\\psi}_{om} \\equiv \\dfrac{1}{n} \\sum_{i=1}^n \\hat{\\mu}_1(L_i) - \\hat{\\mu}_0(L_i),\n\\] and \\[\n\\hat{\\psi}_{ipw} = \\dfrac{1}{n} \\sum_{i=1}^n \\dfrac{A_i}{\\hat{e}(L_i)}Y_i  - \\sum_{i=1}^n \\dfrac{1 - A_i}{1 - \\hat{e}(L_i)}Y_i,\n\\] where the first is termed the outcome model or regression estimator because it relies on a model for \\(E(Y | A = a, L)\\) and the second is the inverse probability weighting estimator because it relies on a model for \\(\\Pr(A = 1 | L)\\).\nHernan and Robins describe, in detail, the necessary analysis steps to obtain estimates \\(\\hat{\\psi}_{om}\\) and \\(\\hat{\\psi}_{ipw}\\). Briefly, the outcome regression estimator is obtained by\n\nRegressing the outcome, \\(Y\\), on covariates, \\(L\\), either a) separately within the subset with \\(A = 1\\) and \\(A = 0\\), or b) by pooling into one model of \\(Y\\) on \\(L\\) and \\(A\\).\nCreate two copies of the dataset and artificially set \\(A = 1\\) for all individuals in one and set \\(A = 0\\) for all individuals in the other.\nObtain predicted values \\(\\hat{\\mu}_1(L_i)\\) and \\(\\hat{\\mu}_0(L_i)\\) by applying the model in step 1 to the observed values of \\(L_i\\) in each dataset.\nTake the mean of these predicted values and subtract them to obtain \\(\\hat{\\psi}_{om}\\).\n\nand the inverse probability weighting estimator may be obtained by\n\nRegressing the treatment, \\(A\\), on covariates, \\(L\\), using, for instance, a logistic model.\nObtain propensity scores \\(\\hat{e}(L_i)\\) from predicted values of logistic model.\nFor each individual form weights based on the inverse probability of the treatment they received \\[\nW_i = \\dfrac{A_i}{\\hat{e}(L_i)}\n\\]\nCalculate the weighted mean and take the difference.\n\n\n\nThe problem\nImplementing\nIn their book, Hernan and Robins suggest variance estimates can be obtained via the bootstrap. However, there are at least two issues. First the bootstrap may be computationally intensive especially as problems become more complex (e.g. once we consider time-varying treatments).\n\n\nThe basics of M-estimation\nThe basic set up is as follows. Let \\(\\theta\\) be a finite dimensional parameter vector. An M-estimator, \\(\\hat{\\theta}\\), for this vector is the solution to sample moment equation \\[\n\\sum_{i=1}^n \\psi(O_i; \\hat{\\theta}) = 0\n\\] where \\(\\psi(O_i; \\hat{\\theta})\\) is often referred to as an estimating function. Under standard regularity conditions, the asymptotic distribution of \\(\\hat{\\theta}\\) is given by \\[\n\\sqrt{n}(\\theta - \\hat{\\theta}) \\overset{d}{\\longrightarrow} N(0, \\mathbb V(\\hat{\\theta}))\n\\] where the asymptotic variance, \\(\\mathbb V_n(\\hat{\\theta})\\), can be estimated from \\[\n\\mathbb V_n(\\hat{\\theta}) = \\mathbb B_n(\\hat{\\theta})^{-1} \\mathbb F_n(\\hat{\\theta})\\{\\mathbb B_n(\\hat{\\theta})^{-1}\\}^T\n\\] with \\[\n\\mathbb B_n(\\hat{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n \\left\\{-\\dfrac{\\partial\\psi(O_i; \\hat{\\theta})}{\\partial\\theta}\\right\\}\n\\] and \\[\n\\mathbb F_n(\\hat{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n \\left\\{\\partial\\psi(O_i; \\hat{\\theta})\\partial\\psi(O_i; \\hat{\\theta})^T\\right\\}\n\\] In M-estimation parlance \\(\\mathbb V_n(\\hat{\\theta})\\) is referred to as the empirical sandwich variance estimator where \\(\\mathbb B_n(\\hat{\\theta})\\) is the “bread” and \\(\\mathbb F_n(\\hat{\\theta})\\) is the “meat”. While analytical solutions can often be derived for the asymptotic variance for a given set of estimating functions, an advantage of M-estimation is that the variance can also be obtained numerically and therefore can be easily extended to any arbitrary set of estimating functions.\nThe beauty of M-estimation is that many common estimation problems can be posed as an estimating function or sequence of estimating functions. Estimating functions can be ``stacked’’\n\nCan I re-express my estimator as a set of stacked estimating equations?\nDo I meet the\n\n\n\nThe geex package\n\nOutcome regression\nWe can convert \\(\\hat{\\psi}_{om}\\) into the following set of stacked estimating equations \\[\n\\psi(O; \\beta_0, \\beta_1, \\psi) = \\begin{bmatrix}\nA \\{Y - \\mu_1(L;\\beta_1)\\} \\\\\n(1 -A) \\{Y - \\mu_0(L;\\beta_0)\\} \\\\\n\\mu_1(L;\\beta_1) - \\mu_0(L;\\beta_1) - \\psi\n\\end{bmatrix}\n\\] where the first two are the score equations for the models \\(\\mu_1(L;\\beta_1)\\) and \\(\\mu_0(L;\\beta_1)\\) respectively and the last obtains the estimate of the difference in predicted values.\nIn the geex package the stacked estimating functions for outcome regression can be implemented as follows.\n\nestfun_or &lt;- function(data, models) {\n  # grab scores \n  m0_scores &lt;- grab_psiFUN(models$m0, data)\n  m1_scores &lt;- grab_psiFUN(models$m1, data)\n\n  # design matrices\n  X0 &lt;- grab_design_matrix(data, grab_fixed_formula(models$m0))\n  X1 &lt;- grab_design_matrix(data, grab_fixed_formula(models$m1))\n\n  # parameter indexes\n  ii0 &lt;- 1:ncol(X0)\n  ii1 &lt;- (ncol(X0) + 1):(ncol(X0) + ncol(X1))\n\n  A &lt;- data$A\n\n  # stacked equations\n  function(theta) {\n    c(m0_scores(theta[ii0]) * (A == 0), \n      m1_scores(theta[ii1]) * (A == 1),\n      X1 %*% theta[ii1] - X0 %*% theta[ii0] - theta[length(theta)])\n  }\n}\n\n\n\nInverse probability weighting\nUsing some probability theory we can show that \\(\\psi_{om}\\) is equal to \\[\n\\psi_{ipw} \\equiv E\\left\\{\\frac{I(A = 1)}{\\Pr(A = 1 \\mid L)} Y\\right\\} - E\\left\\{\\frac{I(A = 0)}{\\Pr(A = 0 \\mid L)} Y\\right\\}\n\\] and simple plug-in estimator\n\\[\n\\hat{\\psi}_{ipw} = \\sum_{i=1}^n \\dfrac{A_i}{\\hat{e}(L_i)}Y_i  - \\sum_{i=1}^n \\dfrac{1 - A_i}{1 - \\hat{e}(L_i)}Y_i.\n\\]\nThe variance is complicated by the estimation of the nuisance terms \\(\\hat{\\mu}_1(L)\\) and \\(\\hat{\\mu}_0(L)\\).\n\\[\n\\psi(O; \\alpha, \\psi) = \\begin{bmatrix}\n\\{A - e(L;\\alpha)\\} L \\\\\n\\dfrac{A}{\\hat{e}(L;\\alpha)}Y  - \\dfrac{1 - A}{1 - \\hat{e}(L;\\alpha)}Y - \\psi\n\\end{bmatrix}\n\\]\n\nestfun_or &lt;- function(data, models) {\n  # grab scores \n  e_scores &lt;- grab_psiFUN(models$e, data)\n\n  # design matrix\n  X &lt;- grab_design_matrix(data, grab_fixed_formula(models$e))\n\n  # parameter indexes\n  ii &lt;- 1:ncol(X)\n\n  A &lt;- data$A\n  Y &lt;- data$Y\n\n  # stacked equations\n  function(theta) {\n    e &lt;- plogis(X %*% theta[ii])\n    c(e_scores(theta[ii]), \n      (A * Y / e) - ((1 - A) * Y / (1 - e)) - theta[length(theta)])\n  }\n}\n\n\n\nG-estimation\n\\[\n\\begin{aligned}\nE\\{Y - E(Y \\mid A = a, X)\\} = 0 \\\\\nE[\\{A - E(A | X)\\}\\{Y - \\psi A\\}] = 0\n\\end{aligned}\n\\]\n\n\n\nPutting it all together\n\nlibrary(geex)\n\n\nAttaching package: 'geex'\n\n\nThe following object is masked from 'package:methods':\n\n    show\n\ngendata &lt;- function(n) {\n  L &lt;- MASS::mvrnorm(\n    n = n,\n    mu = rep(0, 3),\n    Sigma = matrix(\n      data = c(1, 0.3, 0.3,\n               0.3, 1, 0.3,\n               0.3, 0.3, 1), \n      nrow = 3,\n      ncol = 3,\n      byrow = TRUE)\n  )   \n  A &lt;- rbinom(n, 1, plogis(-2 + L[, 1] + sqrt(exp(L[, 2]))+ L[, 3]^2))\n  Y &lt;- rnorm(n, -A + 0.5 * (L[, 1] + L[, 2] + L[, 3] + L[, 1] * L[, 3]))\n  \n  colnames(L) &lt;- paste0(\"L\", 1:3)\n  mat &lt;- data.frame(L, A, Y)\n  return(mat)\n}\n\nestfun &lt;- function(data, models) {\n  m0 &lt;- grab_psiFUN(models$m0, data)\n  m1 &lt;- grab_psiFUN(models$m1, data)\n  X0 &lt;- grab_design_matrix(data, rhs_formula = grab_fixed_formula(models$m0))\n  X1 &lt;- grab_design_matrix(data, rhs_formula = grab_fixed_formula(models$m1))\n  ii0 &lt;- 1:ncol(X0)\n  ii1 &lt;- (ncol(X0) + 1):(ncol(X0) + ncol(X1))\n  \n  A &lt;- data$A\n  function(theta) {\n    c(m0(theta[ii0]) * (A == 0), \n      m1(theta[ii1]) * (A == 1),\n      X1 %*% theta[ii1] - X0 %*% theta[ii0] - theta[length(theta)] )\n  }\n}\n\nd &lt;- gendata(1000)\n\nmodels &lt;- list(\n  m0 = glm(formula = Y ~ L1 + L2 + L3 + L1:L3, \n           family = gaussian,\n           subset = A == 0,\n           data = d),\n  m1 = glm(formula = Y ~ L1 + L2 + L3 + L1:L3, \n           family = gaussian,\n           subset = A == 1,\n           data = d)\n)\n\nnparms &lt;- sum(sapply(models, function(x) length(coef(x)))) + 1\n\nres &lt;- m_estimate(\n  estFUN = estfun,\n  data = d,\n  root_control = setup_root_control(start = rep(0, nparms)),\n  outer_args = list(models = models)\n)\n\nc(\n  roots(res)[11],\n  roots(res)[11] - 1.96 * sqrt(vcov(res)[11,11]),\n  roots(res)[11] + 1.96 * sqrt(vcov(res)[11,11])\n)\n\n[1] -0.9601858 -1.1033235 -0.8170480\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{boyer2024,\n  author = {Boyer, Christopher},\n  title = {M-Estimation for Causal Inference in {R}},\n  date = {2024-02-16},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoyer, Christopher. 2024. “M-Estimation for Causal Inference in\nR.” February 16, 2024."
  },
  {
    "objectID": "posts/2024-02-17-the-many-faces-of-the-g-formula/index.html",
    "href": "posts/2024-02-17-the-many-faces-of-the-g-formula/index.html",
    "title": "The many faces of the g-formula",
    "section": "",
    "text": "We’ve all been there. Just working away on some tricky causal question, minding your own damn business, thinking maybe… just maybe… this time you’ve discovered some new insight and BAM! You realize that it was already covered in Robins 1986.\n\n\n\n\n\n\n\nRobin’s g-computation algorithm, or as it is more commonly known, the g-formula, identifies the causal effect of a time-fixed or time-varying treatment under certain assumptions. It has been used to estimate the effects of interventions on coronary heart disease, respiratory disease, diabetes, mortality, and HIV.\nThe g-formula has several algebraically equivalent representations, namely:\n\nNon-iterated conditional expectation (NICE)\nIterated conditional expectation (ICE)\nInverse probability weighted expectation (IPW)\n\n\nSetup and identification\nTo fix concepts, let’s start with the simple case of a single, binary, time-fixed treatment, \\(A\\), for which we have an outcome, \\(Y\\), measured at the end of follow up as well as covariates \\(L\\) which potentially confound the relationship between \\(A\\) and \\(Y\\). We’d like to estimate the effect of \\(A\\) on \\(Y\\). To help, we define \\(Y^a\\) as an individual’s potential outcome under an intervention which sets \\(A\\) to \\(a\\). Our interest then is in causal effects such as the average treatment effect, i.e. \\[E(Y^{a=1} - Y^{a=0})\\] which can be expressed as a contrast between potential outcomes under different values of treatment.\n\n\n\n\n\n\nA directed acyclic graph for a single, time-fixed treatment.\n\n\n\nTo identify effects, we require the following assumptions\n\nExchangeability: \\[Y^{a} \\perp\\!\\!\\!\\perp A \\mid L\\]\nConsistency: \\[Y^{a} = Y \\text{  if } A = a\\]\nPositivity: \\[1 &gt; \\Pr(A = a \\mid L = l) &gt; 0\\]\n\nThese assumptions are described in exquisite detail in Hernan and Robins, but briefly: the first assumption implies that, conditional on \\(L\\), those who aretreated and untreated are exchangeable with respect to \\(Y\\). Graphically, this condition implies that all backdoor paths between \\(A\\) and \\(Y\\) are blocked by conditioning on \\(L\\), with the crucial assumption being that there are no unmeasured variables \\(U\\) which are related to both \\(A\\) and \\(Y\\) that don’t pass through \\(L\\). The second assumption, consistency, implies that an individual’s observed outcome who received treatment \\(A = a\\) is in fact their potential outcome \\(Y^a\\). While this may seem tautological it may be violated if for instance there are hidden versions of treatment, such as when treatment is ill-defined or when there are network effects such that an individual’s potential outcome is affected by the treatment status of others. Finally, the third assumption, positivity, states that there must be a non-zero probability of observing treatment level \\(A = a\\) within all strata of \\(L\\).\nUnder these assumptions \\(E(Y^a)\\), the mean potential outcome under an intervention that sets \\(A\\) to \\(a\\), is identified by \\[\n\\begin{aligned}\nE(Y^{a}) &\\overset{(1)}{=} \\sum_{l} E(Y^{a} \\mid L = l) \\Pr(L = l) \\\\\n&\\overset{(2)}{=} \\sum_{l} E(Y^{a} \\mid A = a, L = l) \\Pr(L = l) \\\\\n&\\overset{(3)}{=} \\sum_{l} E(Y \\mid A = a, L = l) \\Pr(L = l)\n\\end{aligned}\n\\] where the first line follows from the law of total probability, the second from the independence between \\(Y^a\\) and \\(A\\) after conditioning on \\(L\\), and the third from the consistency of potential outcomes. \\[\nE^{G,a'}(Y) = \\sum_{l} E(Y \\mid A = a', L = l) \\Pr(L = l)\n\\] This is the NICE representation of the g-formula and is perhaps the most well-known. It suggests that the mean potential outcome \\(E(Y^a)\\) is equal to the mean observed outcome among those with observed \\(A\\) equal to \\(a\\) within strata of covariates \\(L\\), necessary to ensure exchangeability, and then standardized tothe marginal (population) distribution of \\(L\\).\nHowever, as shown in the appendix, by applying the rules of probability we find that the NICE expression is also equal to \\[E_{L}\\{E(Y \\mid A = a, L = l)\\}\\] as well as \\[E\\left\\{\\frac{I(A = a)}{\\Pr(A = a \\mid L = l)} Y\\right\\}\\] where the first is the ICE representation of the g-formula, a sequence of iterated expectations with the inner most conditional on the observed treatment \\(A\\) and covariates \\(L\\) and the outer with respect to \\(L\\). The second expression is the IPW representation which weights the outcome in those observed to have \\(A = a\\) by the inverse of the probability of receiving treatment level \\(a\\) conditional on \\(L\\).\n\n\nNonparametric estimation\nIf they are all algebraically equivalent why go through all the fuss? Can’t we just pick one and stick with it? In this section, we’ll see that when \\(A\\) and \\(L\\) are low-dimensional and/or have just a few discrete levels, we can plug-in nonparametric estimates of the components of each and all three estimators will yield the same result, in which case this sentiment may be valid. However, for most real-world applications \\(A\\) and \\(L\\) will be high-dimensional and/or include continuous variables and therefore we will have to specify models for the components, in which case they may diverge in which case each estimator may have different properties.\nTo illustrate, consider a two-time point sequentially randomized trial in which \\(A_0\\), \\(L_1\\), and \\(A_1\\) are binary variables such that we have 8 possible treatment and covariate strata, which can be summarized succinctly in the following Table\n\n\n\n\n\n\\(A_0\\)\n\\(L_1\\)\n\\(A_1\\)\n\\(E(Y\\mid A_0, L_1, A_1)\\)\n\\(N\\)\n\n\n\n\n0\n0\n0\n50\n6000\n\n\n0\n0\n1\n70\n2000\n\n\n0\n1\n0\n200\n2000\n\n\n0\n1\n1\n220\n6000\n\n\n1\n0\n0\n230\n3000\n\n\n1\n0\n1\n250\n1000\n\n\n1\n1\n0\n130\n3000\n\n\n1\n1\n1\n110\n9000\n\n\n\n\n\nwhere \\(N\\) is the number of observations in each strata and \\(E(Y \\mid A_0, L_1, A_1)\\) is the mean outcome in each strata. We’ll ignore sampling variability for now and consider the 32,000 people in this study to constitute the full population. We’d like to estimate the effect comparing the average potential outcomes if everyone were always treated versus never treated, i.e. \\[E(Y^{1,1}) - E(Y^{0,0})\\] To do so, we’ll need estimates of the marginal potential outcomes \\(E(Y^{1,1})\\) and \\(E(Y^{0,0})\\) which are identified by the g-formula. For now let’s focus on \\(E(Y^{1,1})\\) because, once we see an example, obtaining the other is trivial. In this case, because \\(A_0\\), \\(L_1\\), and \\(A_0\\) are low dimensional, we can estimate the NICE, ICE, and IPW representations of the g-formula directly by plugging in estimates of their components.\nFirst, for the NICE plug-in estimator we need estimates of \\(E(Y \\mid A_0 = 1, L_1, A_1 = 1)\\) and \\(\\Pr(L_1 \\mid A_0 = 1)\\). The former can be read directly from the table, the latter we can calculate from the numbers in each strata, i.e. \\[\n\\begin{aligned}\nE(Y^{1,1}) &= \\sum_{l_1}E(Y \\mid A_1 = 1, L_1=l_1, A_0 = 1) \\Pr(L_1 = l_1 \\mid A_0 = 1) \\\\\n&=110 \\cdot \\Pr(L_1 = 1 \\mid A_0 = 1) + 250 \\cdot \\Pr(L_1 = 0 \\mid A_0 = 1) \\\\\n&= 110 \\cdot \\frac{9000 + 3000}{9000 + 3000 + 1000 + 3000} + 250 \\cdot \\frac{1000 + 3000}{9000 + 3000 + 1000 + 3000} \\\\\n&= 145\n\\end{aligned}\n\\] For the ICE plug-in estimator, we again need estimates of \\(E(Y \\mid A_0 = 1, L_1, A_1 = 1)\\), but this time we’ll take a slightly different approach and to estimate the iterated expectation we’ll take a weighted average of the result over the distribution of \\(L_1\\) given \\(A_0 = 1\\), i.e. \\[\n\\begin{aligned}\nE(Y^{1,1}) &= E_{L_1}\\{E(Y \\mid A_1 = 1, L_1=l_1, A_0 = 1) \\mid A_0 = 1\\} = \\\\\n&=\\frac{1}{\\sum_{i=1}^n I(A_0 = 1)} \\sum_{i=1}^n \\bigg\\{ E(Y \\mid A_1 = 1, L_1=1, A_0 = 1) I(A_0 = 1, L_1 = 1) \\\\\n&\\qquad + E(Y \\mid A_1 = 1, L_1=0, A_0 = 1) I(A_0 = 1, L_1 = 1)\\bigg\\} \\\\\n&= \\frac{1}{16000} \\{110 \\cdot (9000 + 3000) + 250 \\cdot (1000 + 3000)\\}\\\\\n&= 145\n\\end{aligned}\n\\] Finally for the IPW estimator, we need estimates of the probabilities of treatment given the past, \\(\\Pr(A_1 = 1 \\mid L_1, A_0 = 1)\\) and \\(\\Pr(A_0 = 1)\\), which again we can calculate directly from the number in each strata. We then estimate the weighted expectation of the outcome weighted by the inverse probabilities of treatment, i.e. \\[\n\\begin{aligned}\nE(Y^{1,1}) &= E\\left\\{ \\frac{I(A_0 = 1, A_1 = 1)}{\\Pr(A_1 = 1 \\mid L_1 = l_1, A_0 = 1) \\Pr(A_0 = 1)} Y\\right\\} = \\\\\n&\\quad= \\frac{1}{n} \\sum_{i=1}^n \\bigg\\{ \\frac{I(A_0 = 1, L_1 = 1, A_1 = 1)}{\\Pr(A_1 = 1 \\mid L_1 = 1, A_0 = 1) \\Pr(A_0 = 1)} Y \\\\\n&\\quad\\quad + \\frac{I(A_0 = 1, L_1 = 0, A_1 = 1)}{\\Pr(A_1 = 1 \\mid L_1 = 0, A_0 = 1) \\Pr(A_0 = 1)} Y\\bigg\\} \\\\\n&\\quad= \\frac{1}{32000}\\left\\{ \\frac{9000}{1/2 \\cdot 3/4}110 + \\frac{1000}{1/2 \\cdot 1/4}250\\right\\}\\\\\n&\\quad= \\frac{1}{32000}\\left\\{ (24000)110 + (8000) 250\\right\\}\\\\\n&\\quad= 145\n\\end{aligned}\n\\]\nwhere \\(\\Pr(A_1 = 1 \\mid L_1 = 1, A_0 = 1) = \\frac{9000}{12000} = \\frac{3}{4}\\), \\(\\Pr(A_1 = 1 \\mid L_1 = 0, A_0 = 1) = \\frac{1000}{4000} = \\frac{1}{4}\\), and \\(\\Pr(A_0 = 1) = \\frac{16000}{32000} = \\frac{1}{2}\\).\nAs expected the estimates from these three approaches coincide exactly when nonparametric estimation is feasible.\n\n\nParametric estimation\n\\[\n\\begin{aligned}\n\\mu_{\\overline{a}_k}(\\overline{l}_k) &= E(Y \\mid \\overline{A}_k = \\overline{a}_k, \\overline{L}_k = \\overline{l}_k) \\\\\n\\lambda(\\overline{l}_k) &= \\Pr(L_k = l_k \\mid \\overline{A}_{k-1} = \\overline{a}_{k-1}, \\overline{L}_{k-1} = \\overline{l}_{k-1}) \\\\\n\\pi(\\overline{l}_k) &= \\Pr(A_k = 1 \\mid \\overline{A}_{k-1} = \\overline{a}_{k-1}, \\overline{L}_k = \\overline{l}_k)\n\\end{aligned}\n\\]\n\\[\\sum_{\\forall \\overline{l}_K}\\mu_{\\overline{a}_K}(\\overline{l}_K; \\widehat{\\beta}) \\prod_{i=1}^K \\lambda(\\overline{l}_i; \\widehat{\\gamma})\\]\n\\[\\frac{1}{n}\\sum_{i=1}^n \\mu_{a_0}(\\ldots \\mu_{a_{K-1}}(\\mu_{\\overline{a}_K}(\\overline{l}_K; \\widehat{\\beta}_K), \\overline{l}_{K-1}; \\widehat{\\beta}_{K-1})\\ldots, l_0; \\widehat{\\beta}_0)\\]\n\\[\\frac{1}{n}\\sum_{i=1}^n\\frac{I(\\overline{A}_K = \\overline{a}_K)}{\\prod_{i=1}^K\\pi(\\overline{l}_i; \\widehat{\\delta})} Y\\]\n\n\nSimulation\n\\[\n\\begin{aligned}\nL_0 &\\sim \\text{Normal}(0, 1) \\\\\nA_0 &\\sim \\text{Bernoulli}(\\operatorname{expit}\\{-3 + L_0 + I(L_0 &gt; 1)\\}) \\\\\nL_1 &\\sim \\text{Normal}(L_0 - A_0, 1) \\\\\nA_1 &\\sim \\text{Bernoulli}(\\operatorname{expit}\\{-3 + 5 A_0 + L_1 + I(L_1 &gt; 1) + I(L_0 &gt; 1)\\}) \\\\\nY &\\sim \\text{Normal}(L_1 + L_0 - A_0 - A_1 - 0.5 A_0 A_1, 1) \\\\\n\\end{aligned}\n\\]\n\\[E(Y^{1, 1} - Y^{0, 0}) = -3.5\\]\nLet’s begin by simulating data from the generative model above in R.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{boyer2024,\n  author = {Boyer, Christopher},\n  title = {The Many Faces of the g-Formula},\n  date = {2024-02-16},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoyer, Christopher. 2024. “The Many Faces of the\ng-Formula.” February 16, 2024."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{boyer2024,\n  author = {Boyer, Christopher and Malloc, Harlow},\n  title = {Post {With} {Code}},\n  date = {2024-02-16},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoyer, Christopher, and Harlow Malloc. 2024. “Post With\nCode.” February 16, 2024."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Pre-prints\n\nBoyer, C., Li, K.Q., Shi, X., & Tchetgen Tchetgen, T. J. (2025). “Identification and estimation of vaccine effectiveness in the test-negative design under equi-confounding”. arXiv. https://doi.org/10.48550/arXiv.2504.20360\nGago, J., Boyer, C., & Lipsitch, M. How should we study the indirect effects of antimicrobial treatment strategies? A causal perspective. medRxiv. https://doi.org/10.1101/2025.03.28.25324855\nBoyer, C., Field, E., Lehrer, R., Morrison, A., & Piras, C. (2025). “Guy Talk: Catalyzing Peer Effects on IPV through Virtual Support Groups for Men”. https://sites.duke.edu/ericafield/files/2025/03/wp_2025_03_Guy_Talk_Peru.pdf\nBoyer, C., Dahabreh, I. J., & Steingrimsson, J. A. (2024). “Estimating and evaluating counterfactual prediction models”. arXiv. https://doi.org/10.48550/arXiv.2308.13026\nZhang, Z., Boyer, C., Lipsitch, M. (2024). “Use of the test-negative design to estimate the protective effect of a scalar immune measure: A simulation analysis”. medRxiv. https://doi.org/10.1101/2024.11.22.24317757\nJoshi, K., Kahn, R., Boyer, C., & Lipsitch, M. (2023). “Some principles for using epidemiologic study results to parameterize transmission models”. medRxiv. https://doi.org/10.1101/2023.10.03.23296455\n\n\n\nPeer-reviewed publications\n\nJia, K. M., Boyer, C., Wallinga, J., & Lipsitch, M. (2025). Causal Estimands for Analyses of Averted and Avertible Outcomes due to Infectious Disease Interventions. Epidemiology, 36(3), 363-373. https://doi.org/10.1097/EDE.0000000000001839   pre-print /  repository /  pdf\nBoyer, C. & Lipsitch, M. (2024). Emulating target trials of postexposure vaccines using observational data. American Journal of Epidemiology. https://doi.org/10.1093/aje/kwae350   pre-print /  repository /  pdf\nBoyer, C., Paluck, E. L., Annan, J., Nevatia, T., Cooper, J., Namubiru, J, Heise, L., & Lehrer, R. (2022) Religious leaders can motivate men to cede power and reduce intimate partner violence: experimental evidence from Uganda. PNAS. 119(31), e2200262119. https://doi.org/10.1073/pnas.2200262119   pre-analysis plan /  repository /  pdf\nBoyer, C., Rumpler, E., Kissler, S. M., & Lipsitch, M. (2022). Infectious disease dynamics and restrictions on social gathering size. Epidemics, 40, 100620. https://doi.org/10.1016/j.epidem.2022.100620   pre-print /  repository /  pdf\nChatterji, S., Boyer, C., Sharma, V., Abramsky, T., Levtov, R., Doyle, K., Harvey, S., & Heise, L. (2023). “Optimizing the Construction of Outcome Measures for Impact Evaluations of Intimate Partner Violence Prevention Interventions.” Journal of Interpersonal Violence, 0(0). https://doi.org/10.1177/08862605231162887   pre-print /  pdf\nRosen, J. B., Arciuolo, R. J., Pathela, P., Boyer, C., Baumgartner, J., Latash, J., Malec, L., Lee, E. H., Reddy, V., King, R., Edward Real, J., Lipsitch, M., & Zucker, J. R. (2024). JYNNEOS™ effectiveness as post-exposure prophylaxis against mpox: Challenges using real-world outbreak data. Vaccine. https://doi.org/10.1016/j.vaccine.2023.12.066   pdf\nNCD Risk Factor Collaboration (NCD-RisC). (2021) “Worldwide trends in hypertension prevalence and progress in treatment and control from 1990 to 2019: a pooled analysis of 1201 population-representative studies with 104 million participants”. The Lancet. 398(10304), 957–980. https://doi.org/10.1016/S0140-6736(21)01330-1   pdf\n\n\n\nDissertation\n\nBoyer, C. (2023) “New approaches to factual and counterfactual prediction modeling”. Doctoral dissertation, Harvard University Graduate School of Arts and Sciences.   pdf /  slides"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Below is a list of courses for which I’ve served as a teaching fellow.\n\nEPI207 - Advanced Epidemiologic Methods\nInstructor: James Robins\nTerms: Fall 2020, Fall 2021 (Lead)\nRequired course for epidemiology PhD students. Causal inference for time-varying exposures: g-formula, inverse-probability weighting, marginal structural models, static and dynamic treatment regimes. Responsible for leading 90 min lab section and grading homeworks and tests.\n\nSlides and code for lab on parametric g-formula.\n\n🏆 Department of Epidemiology Excellence in Teaching Award 2021\n\n\nPHS2000 - Quantitative reseach methods\nInstructors: Tyler Vanderweele, Michael Hughes, Issa Dahabreh, and Jarvis Chen\nTerms: Fall 2019, Spring 2020, Fall 2021\nYear-long required methods course for first-year PhD students. Regression models, sampling, longitudinal and multilevel analysis, time-varying confounding, mediation and interaction, econometric methods, and missing data. Responsible for leading 90 min lab section, developing homework assignments and tests, and drafting course materials.\n\nSlides for bootcamp review on inference.\nSlides and code for bootcamp review on linear regression.\nSlides and code for lab on survival analysis.\nHandout and code for lab on sensitivity analysis.\nHandout and code for lab on causal interaction and effect modification.\nHandout and code for lab on mediation.\nHandout and code for lab on the bootstrap.\n\n🏆 GSAS Distinction in Teaching Award 2019, 2020, and 2021\n\n\nEPI260 - Mathematical Modeling of Infectious Diseases\nInstructor: Marc Lipsitch\nTerms: Spring 2023\nDynamical models to study the transmission dynamics of infectious diseases. Design and construction of appropriate differential equation models, equilibrium and stability analysis, parameter estimation from epidemiological data, determination and interpretation of the basic reproductive number of an infection, stochastic and deterministic models, heterogeneity, techniques for sensitivity analysis, and critique of model assumptions."
  },
  {
    "objectID": "posts/2024-02-26-bayesian-measurement-error-models/index.html",
    "href": "posts/2024-02-26-bayesian-measurement-error-models/index.html",
    "title": "Bayesian measurement error models for longitudinal data",
    "section": "",
    "text": "Recently, I have been working with longitudinal biomarker data that looks something like this:\nViral kinetics fans may notice this looks suspiciously like quantitative PCR results from an acute infection (it is), but for our purposes just note a few salient features:\nI was interested in modeling the underlying biological phenomenon under a Bayesian paradigm, but this was both a slightly more complicated measurement error model than I had seen on the online forums and I had some questions about identifiability. So I thought it might make a nice blog post."
  },
  {
    "objectID": "posts/2024-02-26-bayesian-measurement-error-models/index.html#the-generative-model",
    "href": "posts/2024-02-26-bayesian-measurement-error-models/index.html#the-generative-model",
    "title": "Bayesian measurement error models for longitudinal data",
    "section": "The generative model",
    "text": "The generative model\nA few quick preliminaries, let \\(V_t\\) be the true value of the outcome and \\(V^*_t\\) the observed value both index by time \\(t\\) which is centered at the highest observed value. We’ll assume that, within individuals, the trajectory of the true value is well-approximated by the piece-wise exponential function \\[\n\\log V_t = g(t; \\theta),\n\\] where \\[\ng(t; \\theta) = \\begin{cases}\n   \\dfrac{\\delta}{\\omega_p} (t - (t_p - \\omega_p)) & \\text{if } t \\leq t_p \\\\\n    \\delta - \\dfrac{\\delta}{\\omega_r} (t - t_p) & \\text{if } t &gt; t_p,\n\\end{cases}\n\\] This is essentially a triangle on the log scale with parameters, \\(\\theta = (t_p, \\omega_p, \\delta_p, \\omega_r)\\), governing the time of the peak (\\(t_p\\)), the peak height (\\(\\delta_p\\)), and the rise (\\(\\omega_p\\)) and fall times (\\(\\omega_r\\)).\nWe start by including classical measurement error and assume observed values are normal deviates (on the log scale) about the truth, i.e. \\[\n\\begin{aligned}\n  \\log V^*_t &= \\log V_t + \\varepsilon \\\\\n  \\varepsilon &\\sim N(0, \\sigma)\n\\end{aligned}\n\\] In reality these errors are probably not homoskedastic, but for now this is a reasonable approximation.\nTo incorporate the lower limit of detection of the test, we assume values below the limit are censored such that the distribution function becomes \\[\nf(\\log v^*_t ; \\log v_t, \\sigma) = \\begin{cases}\n\\dfrac{1}{\\sigma \\sqrt{2 \\pi}}\\exp\\left\\{-\\dfrac{1}{2}\\left(\\dfrac{\\log v^*_t-\\log v_t}{\\sigma}\\right)^2\\right\\} & \\text{if }v^*_t &gt; lod \\\\\n\\Phi(\\log v^*_t) & \\text{if } v^* \\leq lod\n\\end{cases}\n\\] where \\(\\Phi(\\cdot)\\) is the cumulative distribution function for the normal distribution. Here we assume that \\(lod\\) is a known fixed value determined by the design of the test and therefore is not a parameter to be estimated from the data. For brevity, we define \\(N(\\log v_t, \\sigma)_{lod}\\) to be the distribution with this distribution function.\nFinally, we include the possibility of false positive and false negatives by assuming the observed is actually drawn from a mixture \\[\nf(\\log v^*_t ; \\log v_t, \\sigma) = \\begin{cases}\np \\cdot \\dfrac{1}{\\sigma \\sqrt{2 \\pi}}\\exp\\left\\{-\\dfrac{1}{2}\\left(\\dfrac{\\log v^*_t-\\log v_t}{\\sigma}\\right)^2\\right\\} + (1-p)& \\text{if }v^*_t &gt; lod \\\\\n\\Phi(\\log v^*_t) & \\text{if } v^* \\leq lod\n\\end{cases}\n\\] \\[\n\\log V^*_t \\sim \\lambda_1 N(\\log V_t, \\sigma)_{lod} + \\lambda_2 Exp(\\mu) + \\lambda_3 \\mathbb I(\\log V^*_t = lod)\n\\] where \\(\\lambda_1\\), \\(\\lambda_2\\), and \\(\\lambda_3\\) are the mixture probabilities denoting when the observed is drawn from the censored classical measurement error distribution (\\(\\lambda_1\\)) or is a false positive (\\(\\lambda_2\\)) or a false negative (\\(\\lambda_3\\)). We assume false positives are exponentially distributed with most of the mass near the limit of detection. False negatives will always have observed values at the lower limit of detection, so here we use the Dirac delta function, \\(\\delta_{V^*_t}(\\cdot)\\), to represent a point mass at the limit.\n\nfunctions {\n  // This function calculates a piece-wise exponential function.\n  // t: The time variable.\n  // tp: The peak time.\n  // wp: The width of the peak.\n  // wr: The width of the right side of the function.\n  // dp: The depth of the peak.\n  real pefun(real t, real tp, real wp, real wr, real dp) {\n    if (t &lt;= tp) {\n      return dp / wp * (t - (tp - wp)); // Viral load rises before peak\n    } else {\n      return dp - dp / wr * (t - tp); // Viral load falls after peak\n    }\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; N;                   // sample size\n  vector[N] v_obs;                  // observed outcome\n  vector[N] t;                      // time\n  real lod;\n}\n\nparameters {\n  real dp;\n  real wp;\n  real wr;\n  real tp;\n  simplex[3] lambda;\n  real sigma;\n}\n\nmodel {\n   // PRIORS\n   beta_t ~ normal(0, 10);\n   beta_c ~ normal(0, 10);\n   alpha_t_raw ~ normal(0, 1);\n   alpha_c_raw ~ normal(0, 1);          \n\n   // LIKELIHOOD \n   for (n in 1:N) {\n     real v = pefun(n, tp, wp, wr, dp);\n     \n     if (v_obs[n] == lod) {\n       target += log_sum_exp(\n         log(lambda),\n         log1m(lambda) + normal_lcdf(lod | v, sigma)\n       )\n     } else {\n       target += log_sum_exp(\n         log(lambda[1]) + normal_lpdf(v_obs[n] | v, sigma), \n         log(lambda[2]) + exponential_lpdf(0 | mu),\n         log(lambda[3]) + dirac\n       )\n     }\n   }\n   y ~ normal(alpha + tau * a + beta_t * a + beta_c * (1 - a), sigma_t * a + sigma_c * (1 - a));\n}"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{boyer2024,\n  author = {Boyer, Christopher and O’Malley, Tristan},\n  title = {Welcome {To} {My} {Blog}},\n  date = {2024-02-13},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoyer, Christopher, and Tristan O’Malley. 2024. “Welcome To My\nBlog.” February 13, 2024."
  },
  {
    "objectID": "posts/2024-02-17-the-reasons-to-start-treatment-are-not-the-same-as-to-stop/index.html",
    "href": "posts/2024-02-17-the-reasons-to-start-treatment-are-not-the-same-as-to-stop/index.html",
    "title": "The reasons people start treatment are not the same as the reasons they stop",
    "section": "",
    "text": "Researchers are often interested in estimating the effects of sustained use of a treatment on a health outcome.\n\\[\\begin{align}\nL_0 \\sim\nA_0 \\sim\nL_1 \\sim\nA_1 \\sim\nY\n\\end{align}\\]\n\n\n\n\n\nA directed acyclic graph for a single, time-fixed treatment.\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{boyer2024,\n  author = {Boyer, Christopher},\n  title = {The Reasons People Start Treatment Are Not the Same as the\n    Reasons They Stop},\n  date = {2024-02-16},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBoyer, Christopher. 2024. “The Reasons People Start Treatment Are\nNot the Same as the Reasons They Stop.” February 16, 2024."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Christopher B. Boyer",
    "section": "",
    "text": "I am currently an Assistant Professor at the Cleveland Clinic Lerner College of Medicine of Case Western Reserve University and a staff Biostatistician at the Lerner Research Institute at the Cleveland Clinic.\nMy research focuses on methods for improving causal estimation and inference in observational and randomized studies. In the past, this has included work counterfactual prediction, postmarket vaccine evaluation, target trial emulation, and interference/spillover and placebo effects in randomized trials. At the Clinic, I collaborate with investigators at the Center for Value Based Care Research leveraging electronic medical records databases to evaluate comparative effectiveness of primary care and hospital-based interventions and as a platform for novel trial design to improve medical decision-making at the point of care.\n\nEducation\n\nPhD, Epidemiology\n\nHarvard University, 2023.\n\nMPH, Epidemiology\n\nColumbia University, 2015.\n\nBS, Mechanical Engineering\n\nWright State University, 2010."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "Below is a list of software that I’ve developed or contributed to:\n\nR\n\ngloborisk. An R package for calculating risks of CVD using Globorisk.\ngmethods. An R package for estimating causal effects using Robins’ “g-methods”, i.e. the g-formula, g-estimation, and inverse probability weighting. (In development)\n\n\n\nStata\n\nipacheck. A Stata package for running high-frequency checks on survey data for field experiments. With Rosemarie Sandino, Ishmail Baako, Caton Brewster, and Isabel Oñate.\nbcstats. A Stata program for analyzing back check (field audit) data and comparing it to the original survey data. With Matthew White.\nsurveycto_api. A Stata program to automate downloading data from SurveyCTO server using the API.\nphotobook. A Stata program to create a nicely formatted photobook from a list of image files."
  }
]